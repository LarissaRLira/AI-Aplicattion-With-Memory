# ü§ñ ChatBot com LangChain e Groq

## üìå Descri√ß√£o
Este projeto implementa um chatbot interativo utilizando **LangChain** e **Groq** para processamento de linguagem natural. Ele permite manter um hist√≥rico de conversa√ß√µes, otimizar o gerenciamento de mem√≥ria e estruturar prompts de forma eficiente.

## üöÄ Tecnologias Utilizadas
- **Python** üêç
- **LangChain** (para cria√ß√£o de fluxos conversacionais)
- **Groq API** (modelo de IA para resposta ao usu√°rio)
- **dotenv** (para gest√£o de vari√°veis de ambiente)

## üìÇ Estrutura do C√≥digo

```python
# Importa√ß√£o das bibliotecas necess√°rias
import os
from dotenv import load_dotenv, find_dotenv
from langchain_groq import ChatGroq  
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, trim_messages
from langchain_core.runnables import RunnablePassthrough
from operator import itemgetter

# Carregar vari√°veis de ambiente
load_dotenv(find_dotenv())
GROQ_API_KEY = os.getenv('GROQ_API_KEY')

# Inicializa o modelo de IA
model = ChatGroq(model="gemma2-9b-it", groq_api_key=GROQ_API_KEY)

# Armazenamento do hist√≥rico
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# Criar gerenciador de hist√≥rico
with_message_history = RunnableWithMessageHistory(model, get_session_history)

# Configurar sess√£o
config = {"configurable": {"session_id": "chat1"}}

# Exemplo de intera√ß√£o inicial
response = with_message_history.invoke([
    HumanMessage(content="Oie, meu nome √© Lari e eu sou engenheira de dados")
], config=config)

print("Resposta do modelo:", response.content)

# Cria√ß√£o de um template de prompt para estruturar a entrada do modelo
prompt = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um assistente √∫til. Responda todas as perguntas com precis√£o no idioma."),
    MessagesPlaceholder(variable_name="messages")  # Permite adicionar mensagens dinamicamente
])

# Conecta o modelo ao template de prompt
chain = prompt | model

# Exemplo de intera√ß√£o usando o template
response = chain.invoke({"messages": [HumanMessage(content="Oi, meu nome √© Lari")]})
print("Resposta do modelo:", response.content)

# Gerenciamento da mem√≥ria do chatbot
trimmer = trim_messages(
    max_tokens=45,  # Define um limite m√°ximo de tokens para evitar ultrapassar o contexto
    strategy="last",  # Mant√©m as √∫ltimas mensagens mais recentes
    token_counter=model,  # Usa o modelo para contar os tokens
    include_system=True,  # Inclui a mensagem do sistema no hist√≥rico
    allow_partial=False,  # Evita que mensagens fiquem cortadas
    start_on="human"  # Come√ßa a contagem com mensagens humanas
)

# Exemplo de hist√≥rico de mensagens
messages = [
    SystemMessage(content="Voc√™ √© um bom assistente"),
    HumanMessage(content="Oi! Meu nome √© Bob"),
    AIMessage(content="Oi, Bob! Como posso te ajudar?"),
    HumanMessage(content="Eu gosto de sorvete de baunilha"),
]

# Aplicar o limitador de mem√≥ria ao hist√≥rico
response = trimmer.invoke(messages)

# Criando um pipeline de execu√ß√£o para otimizar a passagem de informa√ß√µes
chain = (
    RunnablePassthrough.assign(messages=itemgetter("messages") | trimmer)  # Aplica a otimiza√ß√£o do hist√≥rico
    | prompt  # Passa a entrada pelo template de prompt
    | model  # Envia para o modelo
)

# Exemplo de intera√ß√£o utilizando o pipeline otimizado
response = chain.invoke(
    {
        "messages": messages + [HumanMessage(content="Qual sorvete eu gosto?")],
    }
)

# Exibe a resposta final do modelo
print("Resposta final:", response.content)
```

## üõ†Ô∏è Funcionalidades
‚úÖ Integra√ß√£o com a **API do Groq** para respostas inteligentes.  
‚úÖ **Hist√≥rico de conversa√ß√£o** persistente por sess√£o.  
‚úÖ **Gest√£o de mem√≥ria** para otimizar hist√≥rico.  
‚úÖ **Cria√ß√£o de prompts din√¢micos** para intera√ß√µes estruturadas.  
‚úÖ **Pipeline de execu√ß√£o** para melhor desempenho.  

## üîß Como Executar
1. Clone o reposit√≥rio:
   ```bash
   git clone https://github.com/seu-repositorio/chatbot-langchain-groq.git
   ```
2. Acesse o diret√≥rio do projeto:
   ```bash
   cd chatbot-langchain-groq
   ```
3. Crie e ative um ambiente virtual (opcional, mas recomendado):
   ```bash
   python -m venv venv
   source venv/bin/activate  # Para Linux/Mac
   venv\Scripts\activate     # Para Windows
   ```
4. Instale as depend√™ncias:
   ```bash
   pip install -r requirements.txt
   ```
5. Crie um arquivo `.env` e adicione sua chave da API do Groq:
   ```env
   GROQ_API_KEY=your_api_key_here
   ```
6. Execute o c√≥digo:
   ```bash
   python chatbot.py
   ```

## üìå Exemplo de Uso
```python
response = chain.invoke({
    "messages": [HumanMessage(content="Oi, meu nome √© Lari")]
})
print("Resposta do modelo:", response.content)
```

## üéØ Contribui√ß√£o
Sinta-se √† vontade para contribuir! Abra um **issue** ou fa√ßa um **pull request**. üí°

Larissa Souza 

